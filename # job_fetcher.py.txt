# job_fetcher.py
import os
import smtplib
import requests
from bs4 import BeautifulSoup
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from datetime import datetime
from urllib.parse import quote_plus

# Config from env (GitHub secrets or local .env during testing)
SMTP_HOST = os.environ.get("SMTP_HOST")
SMTP_PORT = int(os.environ.get("SMTP_PORT", "587"))
SMTP_USER = os.environ.get("SMTP_USER")
SMTP_PASS = os.environ.get("SMTP_PASS")
TO_EMAIL = os.environ.get("TO_EMAIL")
FROM_EMAIL = os.environ.get("FROM_EMAIL", SMTP_USER)
JOB_ROLES = os.environ.get("JOB_ROLES", "Java Developer,Software Engineer,Frontend Developer,ML Intern").split(",")
LOCATION = os.environ.get("JOB_LOCATION", "")  # leave empty for all India
MAX_PER_SOURCE = int(os.environ.get("MAX_PER_SOURCE", "30"))

USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36"

def fetch_naukri(query, location="", max_results=30):
    """Search Naukri public search page and return listings."""
    results = []
    base = "https://www.naukri.com/"  # Naukri uses JS heavy pages; we use search via query param using advanced search
    # Build search URL using keyword query
    q = quote_plus(query)
    url = f"https://www.naukri.com/{q}-jobs"
    if location:
        url = f"https://www.naukri.com/{q}-jobs-in-{quote_plus(location)}"
    headers = {"User-Agent": USER_AGENT}
    r = requests.get(url, headers=headers, timeout=15)
    if r.status_code != 200:
        return results
    soup = BeautifulSoup(r.text, "lxml")
    job_cards = soup.select("article") or soup.select("div.jobTuple") or soup.select("div.list")
    count = 0
    for card in job_cards:
        if count >= max_results: break
        a = card.find("a", href=True)
        title = card.find("a")
        comp = card.select_one("a.subTitle") or card.select_one("div.companyInfo a") or card.select_one("a[href*='/jobs/']")
        summary = card.get_text(separator=" ", strip=True)[:300]
        link = a["href"] if a else None
        if link and "job" in (link.lower() or ""):
            results.append({
                "source": "Naukri",
                "title": title.get_text(strip=True) if title else "Job",
                "company": comp.get_text(strip=True) if comp else "",
                "link": link,
                "summary": summary
            })
            count += 1
    return results

def fetch_indeed(query, location="", max_results=30):
    results = []
    HEADER = {"User-Agent": USER_AGENT}
    q = quote_plus(query)
    loc = quote_plus(location) if location else ""
    url = f"https://in.indeed.com/jobs?q={q}&l={loc}"
    resp = requests.get(url, headers=HEADER, timeout=15)
    if resp.status_code != 200:
        return results
    soup = BeautifulSoup(resp.text, "lxml")
    cards = soup.select("a.tapItem")
    count = 0
    for c in cards:
        if count >= max_results: break
        title_tag = c.select_one("h2.jobTitle")
        company_tag = c.select_one("span.companyName")
        link = "https://in.indeed.com" + c.get("href") if c.get("href","").startswith("/") else c.get("href")
        summary = c.get_text(separator=" ", strip=True)[:300]
        results.append({
            "source": "Indeed",
            "title": title_tag.get_text(strip=True) if title_tag else "Job",
            "company": company_tag.get_text(strip=True) if company_tag else "",
            "link": link,
            "summary": summary
        })
        count += 1
    return results

def dedupe(jobs):
    seen = set()
    out = []
    for j in jobs:
        key = (j.get("title","").lower(), j.get("company","").lower(), j.get("link",""))
        if key in seen: continue
        seen.add(key)
        out.append(j)
    return out

def build_email_body(jobs):
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    html = f"<h2>Daily Fresher Jobs — {now} (Top {len(jobs)})</h2>"
    html += "<p>Open in browser to apply. Use your autofill / Simplify extension to speed up applying.</p>"
    html += "<ol>"
    for j in jobs:
        html += "<li>"
        html += f"<b>{j['title']}</b> — {j['company']} <br/>"
        html += f"{j['summary'][:220]}...<br/>"
        html += f"<a href='{j['link']}' target='_blank'>Open job & apply</a> — Source: {j['source']}"
        html += "</li><br/>"
    html += "</ol>"
    return html

def send_email(subject, html_body):
    msg = MIMEMultipart("alternative")
    msg["Subject"] = subject
    msg["From"] = FROM_EMAIL
    msg["To"] = TO_EMAIL

    part = MIMEText(html_body, "html")
    msg.attach(part)

    server = smtplib.SMTP(SMTP_HOST, SMTP_PORT, timeout=30)
    server.starttls()
    server.login(SMTP_USER, SMTP_PASS)
    server.sendmail(FROM_EMAIL, [TO_EMAIL], msg.as_string())
    server.quit()

def main():
    all_jobs = []
    for role in JOB_ROLES:
        role = role.strip()
        if not role:
            continue
        try:
            naukri = fetch_naukri(role + " fresher", LOCATION, MAX_PER_SOURCE)
            indeed = fetch_indeed(role + " fresher", LOCATION, MAX_PER_SOURCE)
            all_jobs.extend(naukri)
            all_jobs.extend(indeed)
        except Exception as e:
            print("Error fetching for role", role, e)
    all_jobs = dedupe(all_jobs)
    # Limit total
    if len(all_jobs) > 100:
        all_jobs = all_jobs[:100]
    if not all_jobs:
        html = "<p>No jobs found today matching your roles. Try widening keywords.</p>"
        send_email("Daily Fresher Jobs — 0 results", html)
        return
    body = build_email_body(all_jobs)
    send_email("Daily Fresher Jobs — " + str(len(all_jobs)) + " matches", body)
    print("Email sent with", len(all_jobs), "jobs")

if __name__ == "__main__":
    main()
